{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fakenews-pyspark-bert-train+infer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOjRdQ57yEBYnudBuPbZUBe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymkge/competiton/blob/main/fakenews_pyspark_bert_train%2Binfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iH22PYSg1uI5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Colab Setup"
      ],
      "metadata": {
        "id": "4USlA5Qw12Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q pyspark==3.2.0 spark-nlp==3.4.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uH62eFnw13ej",
        "outputId": "5f6a7a71-6621-46eb-e272-4b480fa896f9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.3 MB 38 kB/s \n",
            "\u001b[K     |████████████████████████████████| 142 kB 16.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 44.2 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start(gpu = True, spark32=True) # for GPU training >> sparknlp.start(gpu = True) # for Spark 2.3 =>> sparknlp.start(spark23 = True)\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "print(\"Spark NLP version\", sparknlp.version())\n",
        "print(\"Apache Spark version:\", spark.version)\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "bDEJxCqs15R4",
        "outputId": "5dcc4d45-5c33-4794-9281-88a0107096c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version 3.4.2\n",
            "Apache Spark version: 3.2.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ff7cda5d810>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://956499504d40:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ujGMkQP1_s3",
        "outputId": "be34d8c8-9f05-48d4-fb01-dd51450df611"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "kNb3GrSK2Bh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    # Globals #\n",
        "    EXP_ID = 'EXP_010' \n",
        "    seed = 111"
      ],
      "metadata": {
        "id": "Dr_krr8v2CC4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Path\n",
        "import os\n",
        "\n",
        "ROOT =       '/content/drive/MyDrive/Notebooks/competition/fake_news_detection'\n",
        "CUR_DIR =    ROOT + '/01_code/'\n",
        "DATA_DIR =   ROOT + '/00_data/'\n",
        "MODEL_DIR =  ROOT + '/02_model/'\n",
        "LOG_DIR =    ROOT + f'/02_model/{CFG.EXP_ID}/log/'\n",
        "RESULT_DIR = ROOT + '/03_result/'\n",
        "\n",
        "if not os.path.exists(LOG_DIR):\n",
        "    os.makedirs(LOG_DIR)\n",
        "\n",
        "%cd $CUR_DIR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIzXfDmw2FRY",
        "outputId": "cf2b8693-5ecc-477a-b01a-dcd888b80e14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Notebooks/competition/fake_news_detection/01_code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp_R-Gar3sK-",
        "outputId": "9e2a469f-a029-4787-e26d-3f7a67c93460"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Notebooks/competition/fake_news_detection/01_code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "Osf8UUdx2HAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainDataset = spark.read.option(\"header\", True).csv(DATA_DIR + 'train.csv')\n",
        "holdout = spark.read.option(\"header\", True).csv(DATA_DIR + 'test.csv')\n",
        "sample = spark.read.option(\"header\", True).csv(DATA_DIR + 'sample_submission.csv')\n",
        "\n",
        "trainDataset.show(truncate=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx-B4EBK2HoQ",
        "outputId": "576804a1-0c19-443c-b43a-4bf336bfa139"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-------------------------------------------------------------------------------------------------+\n",
            "|        id|isFake|                                                                                             text|\n",
            "+----------+------+-------------------------------------------------------------------------------------------------+\n",
            "|d19828eb64|     1|       Cによると、アメリカの元大統領で、最長寿だったジョージ・ウォーカー・ブッシュ氏が27日(C-5...|\n",
            "|dfaab096bd|     0|    中日新聞によると、コナミカップ・プロ野球アジアシリーズ2007の決勝戦・日本の中日ドラゴンズ対...|\n",
            "|163504bf95|     1|      愛媛Cは、11月12日にリーグ準加盟の承認を受けて、来期リーグ加盟を目指す愛媛Cに対して、鈴木...|\n",
            "|ed3c9dc579|     0|         国民日報によると3日、7時50分（UTC+9、日本時間と同じ）大韓民国京畿道平沢市の西海岸（ソ...|\n",
            "|e06f88267f|     1|   共同通信によると、5日午後2時過ぎから東京都、神奈川県、千葉県の3都県の広い範囲の地域で停電が...|\n",
            "|2f5903a788|     0|                2005年12月31日の河北新報、日刊スポーツによると、同年12月30日深夜10時35分ごろ、...|\n",
            "|b2ca4f9386|     1|        日本経済新聞によると、日本バスケットボール協会は4月3日、2007年10月の開幕を目指す協会主...|\n",
            "|ea52aa5790|     0|          神奈川新聞によると30日(UTC+9)、横浜市北部に同市営地下鉄の新線「グリーンライン（4号線...|\n",
            "|a994696540|     0|     マイナビニュースとMdNによると、アイ・オー・データ機器より新モデルのネットワークカメラを20...|\n",
            "|cb6ea64e2e|     0|     朝日新聞によると、7月16日に起きた新潟県中越沖地震のため試合の開催を自粛している第89回全国...|\n",
            "|675b544c61|     1|  毎日新聞によると、サッカー日本代表の監督や、国際サッカー連盟副会長も努めた長沼健さんが6月2日...|\n",
            "|d70ada7579|     1|参議院本会議で郵政民営化法案が否決されたことを受け、自由民主党の石破茂幹事長は民主党・社民党を...|\n",
            "|a07cf27590|     0|              NHKによると、2011年4月に民放で、2012年1月にはEテレでも再編集して放送されたアニメ...|\n",
            "|b8dc2c143b|     0|  気象庁は30日午後、山口県を除いた中国・近畿・東海・北陸・関東甲信の各地方で梅雨明けしたとみら...|\n",
            "|2a50a99e1b|     1|   楽天株式会社は7月23日、同社運営のインターネットショッピングサイト「楽天市場」に出店している...|\n",
            "|f20d84bdd5|     1|     47によると、4月2日、川崎市多摩区のマンションで起きた女性襲撃と小学校3年生の男児の転落死事...|\n",
            "|43a0298ed7|     1|               朝日新聞によると、松下電器は、ブランドを「anasonic」から「ational」へ一本化する...|\n",
            "|94f2378317|     1|       産経新聞・読売新聞・朝日新聞などによると7月22日午後8時40分ごろ、東京都八王子市明神町3の...|\n",
            "|1e2ea2a0a8|     0|           ぷらっとホームは、同社取締役会長の本多弘男さんが6月6日 (UTC+9) に亡くなっていたこと...|\n",
            "|229d778e0f|     1|       によると、フランス南東部アン県で22日(現地時刻、C+2)、武装した集団が銃を乱射し、少なくと...|\n",
            "+----------+------+-------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "print(f\"Total {trainDataset.count()}\")\n",
        "\n",
        "trainDataset.groupBy(\"isFake\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iO85Tfe3v-p",
        "outputId": "30948c50-b41c-4e2b-9b43-55a26c15f698"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 3781\n",
            "+------+-----+\n",
            "|isFake|count|\n",
            "+------+-----+\n",
            "|     1| 1937|\n",
            "|     0| 1844|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed for reproducibility\n",
        "(trainData, testData) = trainDataset.randomSplit([0.8, 0.2], seed = CFG.seed)\n",
        "print(\"Train Dataset Count: \" + str(trainData.count()))\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zChbmD9u31FP",
        "outputId": "926e40ee-64dd-4b63-9a59-b3edd2513530"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Count: 3031\n",
            "Test Dataset Count: 750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Pipeline"
      ],
      "metadata": {
        "id": "kcodTvj_359w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "word_segmenter = WordSegmenterModel.pretrained('wordseg_gsd_ud', 'ja')\\\n",
        "        .setInputCols([\"document\"])\\\n",
        "        .setOutputCol(\"token\") \n",
        "\n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner.pretrained(\"stopwords_iso\", \"ja\")\\\n",
        "    .setInputCols(\"normalized\")\\\n",
        "    .setOutputCol(\"cleanTokens\")\\\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained(\"lemma\", \"ja\") \\\n",
        "        .setInputCols([\"cleanTokens\"]) \\\n",
        "        .setOutputCol(\"lemma\")\n",
        "\n",
        "# bert_embeddings = BertEmbeddings().pretrained(name=\"bert_base_japanese\", lang=\"ja\") \\\n",
        "#     .setInputCols([\"document\",'cleanTokens'])\\\n",
        "#     .setOutputCol(\"embeddings\")\n",
        "\n",
        "### bert pretrained \n",
        "# name='small_bert_L4_256', lang='en'\n",
        "# name='sent_small_bert_L8_512', lang='en'\n",
        "# name='bert_embeddings_bert_base_ja_cased', lang='ja'\n",
        "# name=\"bert_base_japanese\", lang=\"ja\"\n",
        "\n",
        "\n",
        "albert_embeddings = AlbertEmbeddings.pretrained(\"albert_embeddings_albert_base_japanese_v1\",\"ja\") \\\n",
        "    .setInputCols([\"document\", \"lemma\"]) \\\n",
        "    .setOutputCol(\"embeddings\")\n",
        "\n",
        "embeddingsSentence = SentenceEmbeddings() \\\n",
        "    .setInputCols([\"document\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"sentence_embeddings\") \\\n",
        "    .setPoolingStrategy(\"AVERAGE\")\n",
        "\n",
        "classsifierdl = ClassifierDLApproach()\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"class\")\\\n",
        "    .setLabelColumn(\"isFake\")\\\n",
        "    .setMaxEpochs(30)\\\n",
        "    .setLr(0.002)\\\n",
        "    .setDropout(0.5)\\\n",
        "    .setBatchSize(4)\\\n",
        "    .setEnableOutputLogs(True)\\\n",
        "    .setRandomSeed(CFG.seed)\\\n",
        "    .setOutputLogsPath(LOG_DIR)\n",
        "\n",
        "# ClassifierDLApproach(Default): lr=0.005, batchSize=64, dropou=0.5, maxEpochs=30\n",
        "\n",
        "bert_clf_pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    word_segmenter,\n",
        "    normalizer,\n",
        "    stopwords_cleaner,\n",
        "    lemmatizer,\n",
        "    albert_embeddings,\n",
        "    embeddingsSentence,\n",
        "    classsifierdl\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SedTRhL3330",
        "outputId": "71e14899-7e05-4b17-a863-c52b6a1b4369"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wordseg_gsd_ud download started this may take some time.\n",
            "Approximate size to download 979 KB\n",
            "[OK!]\n",
            "stopwords_iso download started this may take some time.\n",
            "Approximate size to download 1.8 KB\n",
            "[OK!]\n",
            "lemma download started this may take some time.\n",
            "Approximate size to download 3.4 MB\n",
            "[OK!]\n",
            "albert_embeddings_albert_base_japanese_v1 download started this may take some time.\n",
            "Approximate size to download 43.5 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform \n",
        "\n",
        "finisher = Finisher() \\\n",
        "    .setInputCols([\"lemma\"]) \\\n",
        "    .setOutputCols([\"tokens\"]) \\\n",
        "    .setOutputAsArray(True) \\\n",
        "    .setCleanAnnotations(False)\n",
        "\n",
        "transform_pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    word_segmenter,\n",
        "    normalizer,\n",
        "    stopwords_cleaner,\n",
        "    lemmatizer, \n",
        "    finisher\n",
        "])\n",
        "\n",
        "transform_pipeline_run = transform_pipeline.fit(trainData)\n",
        "transform_df = transform_pipeline_run.transform(trainData)\n",
        "\n",
        "transform_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5hFKNgp3-OB",
        "outputId": "e8673700-93f7-4734-a89c-da80ad3fec56"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-------------------------------------+--------------------+----------------------+----------------------+----------------------+----------------------+-------------------------------+\n",
            "|        id|isFake|                                 text|            document|                 token|            normalized|           cleanTokens|                 lemma|                         tokens|\n",
            "+----------+------+-------------------------------------+--------------------+----------------------+----------------------+----------------------+----------------------+-------------------------------+\n",
            "|0013fa1710|     1|      スポーツ仲裁裁判所(C)が2月11...|[{document, 0, 38...|[{token, 0, 3, スポ...|[{token, 0, 3, スポ...|[{token, 0, 3, スポ...|[{token, 0, 3, スポ...|   [スポーツ, 仲, 裁, 裁, 判...|\n",
            "|0038263cc9|     0|」というクレジットのもとに「アメリ...|[{document, 0, 38...| [{token, 0, 0, 」,...| [{token, 1, 1, と,...|[{token, 4, 8, クレ...|[{token, 4, 8, クレ...|[クレジット, アメリカ, 国防,...|\n",
            "|00629d85f8|     0|     朝日放送によると、11月24日夜7...|[{document, 0, 77...|[{token, 0, 1, 朝日...|[{token, 0, 1, 朝日...|[{token, 0, 1, 朝日...|[{token, 0, 1, 朝日...|    [朝日, 放送, 月, 日, 夜,...|\n",
            "|0095046d02|     1|エジプトのシナイ半島での爆弾テロ事...|[{document, 0, 60...|[{token, 0, 3, エジ...|[{token, 0, 3, エジ...|[{token, 0, 3, エジ...|[{token, 0, 3, エジ...| [エジプト, シナイ, 半島, 爆...|\n",
            "|00a34678bb|     1|朝日新聞、共同通信によると、みずほ...|[{document, 0, 64...|[{token, 0, 1, 朝日...|[{token, 0, 1, 朝日...|[{token, 0, 1, 朝日...|[{token, 0, 1, 朝日...|   [朝日, 新聞, 共同, 通信, ...|\n",
            "|00c08d89fe|     0| 日刊スポーツによると、野球殿堂の2...|[{document, 0, 39...|[{token, 0, 1, 日刊...|[{token, 0, 1, 日刊...|[{token, 0, 1, 日刊...|[{token, 0, 1, 日刊...| [日刊, スポーツ, 野球, 殿堂...|\n",
            "|00cc75b3c3|     1|セイジ・オザワ松本フェスティバル実...|[{document, 0, 45...|[{token, 0, 2, セイ...|[{token, 0, 2, セイ...|[{token, 0, 2, セイ...|[{token, 0, 2, セイ...|   [セイジ, オザワ, 松, 本, ...|\n",
            "|00e2c609b4|     0|       宇宙航空研究開発機構 (JAXA)...|[{document, 0, 14...|[{token, 0, 1, 宇宙...|[{token, 0, 1, 宇宙...|[{token, 0, 1, 宇宙...|[{token, 0, 1, 宇宙...|   [宇宙, 航空, 研究, 開発, ...|\n",
            "|00e6b84936|     1|朝日新聞などが伝えたところによると...|[{document, 0, 39...|[{token, 0, 1, 朝日...|[{token, 0, 1, 朝日...|[{token, 0, 1, 朝日...|[{token, 0, 1, 朝日...|   [朝日, 新聞, 伝える, 年, ...|\n",
            "|00f7f5c26f|     0|       2016年5月25日に安倍晋三総理...|[{document, 0, 38...|  [{token, 0, 3, 20...| [{token, 4, 4, 年,...| [{token, 4, 4, 年,...| [{token, 4, 4, 年,...|     [年, 月, 日, 安倍, 晋, ...|\n",
            "|00fcb2d718|     1|産経新聞によると、ロッテ滋賀工場で...|[{document, 0, 48...|[{token, 0, 1, 産経...|[{token, 0, 1, 産経...|[{token, 0, 1, 産経...|[{token, 0, 1, 産経...|  [産経, 新聞, ロッテ, 滋賀,...|\n",
            "|010b9063e5|     0|     7日午後10時41分ごろ、千葉県北...|[{document, 0, 34...|  [{token, 0, 0, 7,...| [{token, 1, 1, 日,...| [{token, 1, 1, 日,...| [{token, 1, 1, 日,...|   [日, 午後, 時, 分ごろ, 千...|\n",
            "|011ba6714b|     0|  24日、ソニーの中川裕副社長が会見...|[{document, 0, 26...|  [{token, 0, 1, 24...| [{token, 2, 2, 日,...| [{token, 2, 2, 日,...| [{token, 2, 2, 日,...|   [日, ソニー, 中川, 裕, 副...|\n",
            "|012b946b10|     0|テレビゲーム周辺機器を開発している...|[{document, 0, 48...|[{token, 0, 2, テレ...|[{token, 0, 2, テレ...|[{token, 0, 2, テレ...|[{token, 0, 2, テレ...| [テレビ, ゲーム, 周辺, 機器...|\n",
            "|01375f5365|     1|毎日新聞によると、スーダン暫定政府...|[{document, 0, 31...|[{token, 0, 1, 毎日...|[{token, 0, 1, 毎日...|[{token, 0, 1, 毎日...|[{token, 0, 1, 毎日...| [毎日, 新聞, スーダン, 暫定...|\n",
            "|015dd044ee|     1|中日新聞によると、宮崎県の安藤忠恕...|[{document, 0, 50...|[{token, 0, 1, 中日...|[{token, 0, 1, 中日...|[{token, 0, 1, 中日...|[{token, 0, 1, 中日...|   [中日, 新聞, 宮崎, 県, 安...|\n",
            "|017ae98719|     0|   国際連合の総会は、13日午後3時（...|[{document, 0, 20...|[{token, 0, 1, 国際...|[{token, 0, 1, 国際...|[{token, 0, 1, 国際...|[{token, 0, 1, 国際...|   [国際, 連合, 総会, 日, 午...|\n",
            "|019209346a|     0|朝日新聞・読売新聞によると、広島と...|[{document, 0, 79...|[{token, 0, 1, 朝日...|[{token, 0, 1, 朝日...|[{token, 0, 1, 朝日...|[{token, 0, 1, 朝日...|   [朝日, 新聞, 読売, 新聞, ...|\n",
            "|01b1ae255c|     0|徳島新聞、読売新聞によると、日本の...|[{document, 0, 38...|[{token, 0, 1, 徳島...|[{token, 0, 1, 徳島...|[{token, 0, 1, 徳島...|[{token, 0, 1, 徳島...|   [徳島, 新聞, 読売, 新聞, ...|\n",
            "|01b23136ea|     0|       秋田魁新報、2005年9月27日の...|[{document, 0, 42...|[{token, 0, 1, 秋魁...|[{token, 0, 1, 秋魁...|[{token, 0, 1, 秋魁...|[{token, 0, 1, 秋魁...|    [秋魁, 田, 新報, 年, 月,...|\n",
            "+----------+------+-------------------------------------+--------------------+----------------------+----------------------+----------------------+----------------------+-------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "wZ7Mp6ZP4EKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove the existing logs\n",
        "\n",
        "! rm -r {LOG_DIR}"
      ],
      "metadata": {
        "id": "5ZJJ5fO04Elk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# training will take some time due to Bert (use GPU runtime when possible)\n",
        "\n",
        "bert_clf_pipelineModel = bert_clf_pipeline.fit(trainData)"
      ],
      "metadata": {
        "id": "yb-0cQ6c4I7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check log file\n",
        "\n",
        "log_files = os.listdir(f'{LOG_DIR}')\n",
        "log_files"
      ],
      "metadata": {
        "id": "D_4c_vdh4K5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read log file\n",
        "\n",
        "log_file_name = os.listdir(f'{LOG_DIR}')[0]\n",
        "\n",
        "with open(f'{LOG_DIR}{log_file_name}', \"r\") as log_file :\n",
        "    print(log_file.read())"
      ],
      "metadata": {
        "id": "-QU3DieD4LO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = bert_clf_pipelineModel.transform(testData)\n",
        "preds_df = preds.select('isFake','text',\"class.result\").toPandas()\n",
        "\n",
        "print(preds_df)"
      ],
      "metadata": {
        "id": "U0MPBdTV4LRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are going to use sklearn to evalute the results on test dataset\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "preds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n",
        "\n",
        "print (classification_report(preds_df['result'], preds_df['isFake']))"
      ],
      "metadata": {
        "id": "vo1XhAvO4PaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JojCQDay4PcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "lZb2KlPR4TC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save a Spark NLP pipeline\n",
        "bert_clf_pipelineModel.save(f'{MODEL_DIR}{CFG.EXP_ID}/model')"
      ],
      "metadata": {
        "id": "b5c3s4nj4Pej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "t_f70l4A4YY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting (Colab Setup to Load Dataset)\n",
        "\n",
        "loaded_bert_clf_pipelineModel = PipelineModel.load(f'{MODEL_DIR}{CFG.EXP_ID}/model')"
      ],
      "metadata": {
        "id": "vmD_N-7F4ZV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NAQc6psd4VIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infer"
      ],
      "metadata": {
        "id": "VRcZLd0x4hdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# submission\n",
        "\n",
        "preds = loaded_bert_clf_pipelineModel.transform(holdout)\n",
        "\n",
        "preds_df = preds.select('id', 'text',\"class.result\").toPandas()\n",
        "\n",
        "print(preds_df)"
      ],
      "metadata": {
        "id": "4PNEhN784VK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create submission_df\n",
        "submission_df = preds_df[['id', 'result']].copy()\n",
        "\n",
        "submission_df['result'] = submission_df['result'].str[0]\n",
        "submission_df = submission_df.rename(columns={\"result\": \"isFake\"})\n",
        "\n",
        "print(len(submission_df)) # Recheck 3781\n",
        "\n",
        "submission_df"
      ],
      "metadata": {
        "id": "IkULUM_Q4lUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save file\n",
        "\n",
        "SAVE_DIR = RESULT_DIR + f'submission_{CFG.EXP_ID}.csv'\n",
        "submission_df.to_csv(SAVE_DIR, index=False)\n",
        "\n",
        "print(SAVE_DIR)"
      ],
      "metadata": {
        "id": "kgOY_dX14qVR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}